{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext==0.16.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.16.0)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchtext==0.16.0) (4.66.4)\n",
      "Requirement already satisfied: requests in /Users/nazarlenisin/Library/Python/3.11/lib/python/site-packages (from torchtext==0.16.0) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchtext==0.16.0) (2.1.0)\n",
      "Requirement already satisfied: numpy in /Users/nazarlenisin/Library/Python/3.11/lib/python/site-packages (from torchtext==0.16.0) (1.26.1)\n",
      "Requirement already satisfied: torchdata==0.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchtext==0.16.0) (0.7.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.1.0->torchtext==0.16.0) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.1.0->torchtext==0.16.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.1.0->torchtext==0.16.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.1.0->torchtext==0.16.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/nazarlenisin/Library/Python/3.11/lib/python/site-packages (from torch==2.1.0->torchtext==0.16.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.1.0->torchtext==0.16.0) (2024.6.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in /Users/nazarlenisin/Library/Python/3.11/lib/python/site-packages (from torchdata==0.7.0->torchtext==0.16.0) (2.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nazarlenisin/Library/Python/3.11/lib/python/site-packages (from requests->torchtext==0.16.0) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nazarlenisin/Library/Python/3.11/lib/python/site-packages (from requests->torchtext==0.16.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nazarlenisin/Library/Python/3.11/lib/python/site-packages (from requests->torchtext==0.16.0) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/nazarlenisin/Library/Python/3.11/lib/python/site-packages (from jinja2->torch==2.1.0->torchtext==0.16.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch==2.1.0->torchtext==0.16.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchtext==0.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: portalocker==2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.8.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install portalocker==2.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchtext.datasets.UDPOS(split = 'train')\n",
    "test_dataset = torchtext.datasets.UDPOS(split = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = []\n",
    "train_tags = []\n",
    "\n",
    "for data_idx,data in enumerate(train_dataset):\n",
    "  train_text.append(data[0])\n",
    "  train_tags.append(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = []\n",
    "test_tags = []\n",
    "\n",
    "for data_idx,data in enumerate(test_dataset):\n",
    "  test_text.append(data[0])\n",
    "  test_tags.append(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Tokenizer:\n",
    "  def __init__(self,nlp):\n",
    "    self.nlp = nlp\n",
    "\n",
    "  def _spacy_tokenizer(self,doc):\n",
    "    doc = ' '.join(doc)\n",
    "    return [token.lemma_ for token in self.nlp(doc)]\n",
    "\n",
    "  def _yield_tokens(self,doc):\n",
    "    for text in doc:\n",
    "      text = self._spacy_tokenizer(text)\n",
    "      yield text\n",
    "\n",
    "  def _vocab(self,doc):\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        self._yield_tokens(doc),\n",
    "        specials = ['<pad>','<unk>']\n",
    "    )\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "    self.vocab = vocab\n",
    "\n",
    "    return vocab\n",
    "\n",
    "  def tokenize(self,doc,maxlen,vocab = None):\n",
    "    if vocab == None:\n",
    "      self._vocab(doc)\n",
    "\n",
    "    transforms = T.Sequential(\n",
    "        T.VocabTransform(self.vocab),\n",
    "        T.Truncate(max_seq_len = maxlen),\n",
    "        T.ToTensor(padding_value = 0),\n",
    "        T.PadTransform(max_length = maxlen,pad_value = 0)\n",
    "    )\n",
    "\n",
    "    output = np.array([transforms(text) for text in doc])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagging_rnn(torch.nn.Module):\n",
    "  def __init__(self,sequence_length,text_vocab_size,embedding_size,hidden_size,num_layers,tags_vocab_size):\n",
    "    super().__init__()\n",
    "    self.sequence_length = sequence_length\n",
    "    self.text_vocab_size = text_vocab_size\n",
    "    self.embedding_size = embedding_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "    self.tags_vocab_size = tags_vocab_size\n",
    "\n",
    "    self.embedding = torch.nn.Embedding(text_vocab_size,embedding_size,padding_idx = 0)\n",
    "    self.rnn = torch.nn.RNN(embedding_size,hidden_size,num_layers,batch_first = True,bidirectional = True)\n",
    "\n",
    "\n",
    "    self.linear_1 = torch.nn.Linear(in_features = 2 * hidden_size , out_features = 128)\n",
    "    self.linear_2 = torch.nn.Linear(in_features = 128 , out_features = tags_vocab_size)\n",
    "    self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "  def forward(self,X):\n",
    "    X = self.embedding(X)\n",
    "\n",
    "    output,_ = self.rnn(X)\n",
    "\n",
    "\n",
    "    output = self.linear_1(output)\n",
    "    output = self.sigmoid(output)\n",
    "\n",
    "    output = self.linear_2(output)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagging_lstm(torch.nn.Module):\n",
    "  def __init__(self,sequence_length,text_vocab_size,embedding_size,hidden_size,num_layers,tags_vocab_size):\n",
    "    super().__init__()\n",
    "    self.sequence_length = sequence_length\n",
    "    self.text_vocab_size = text_vocab_size\n",
    "    self.embedding_size = embedding_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "    self.tags_vocab_size = tags_vocab_size\n",
    "\n",
    "    self.embedding = torch.nn.Embedding(text_vocab_size,embedding_size,padding_idx = 0)\n",
    "    self.lstm = torch.nn.LSTM(embedding_size,hidden_size,num_layers,batch_first = True,bidirectional = True)\n",
    "\n",
    "\n",
    "    self.linear_1 = torch.nn.Linear(in_features = 2 * hidden_size , out_features = 128)\n",
    "    self.linear_2 = torch.nn.Linear(in_features = 128 , out_features = tags_vocab_size)\n",
    "    self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "  def forward(self,X):\n",
    "\n",
    "    X = self.embedding(X)\n",
    "\n",
    "    output,_ = self.lstm(X)\n",
    "\n",
    "    output = self.linear_1(output)\n",
    "    output = self.sigmoid(output)\n",
    "\n",
    "    output = self.linear_2(output)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Model:\n",
    "  def __init__(self,model,loss_function,optimizer,epochs):\n",
    "    self.model = model\n",
    "    self.loss_function = loss_function\n",
    "    self.optimizer = optimizer\n",
    "    self.epochs = epochs\n",
    "\n",
    "  def fit(self,train_dataset_batched):\n",
    "    from sklearn.metrics import accuracy_score as accuracy\n",
    "    from tqdm.auto import tqdm\n",
    "\n",
    "    model = self.model\n",
    "\n",
    "    for epoch in tqdm(range(self.epochs)):\n",
    "      print(f'\\n')\n",
    "      model.train()\n",
    "\n",
    "      train_batch_loss = 0\n",
    "      train_batch_acc = 0\n",
    "\n",
    "      for batch,(text,tag) in tqdm(enumerate(train_dataset_batched)):\n",
    "        train_prediction = model(text).permute(0,2,1)\n",
    "        train_labels = train_prediction.argmax(1)\n",
    "\n",
    "        train_loss = self.loss_function(train_prediction,tag)\n",
    "        train_acc = accuracy(tag.flatten(),train_labels.flatten())\n",
    "\n",
    "        train_batch_loss += train_loss\n",
    "        train_batch_acc += train_acc\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "      train_batch_loss /= len(train_dataset_batched)\n",
    "      train_batch_acc /= len(train_dataset_batched)\n",
    "\n",
    "      print(f'Epoch: {epoch} | Train Loss: {train_batch_loss} | Train Accuracy: {train_batch_acc}')\n",
    "\n",
    "  def eval(self,test_dataset_batched):\n",
    "    from sklearn.metrics import accuracy_score as accuracy\n",
    "    from tqdm.auto import tqdm\n",
    "    model = self.model\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_batch_loss = 0\n",
    "    test_batch_acc = 0\n",
    "\n",
    "    for batch,(text,tag) in enumerate(test_dataset_batched):\n",
    "      test_prediction = model(text).permute(0,2,1)\n",
    "      test_labels = test_prediction.argmax(1)\n",
    "\n",
    "      test_loss = self.loss_function(test_prediction,tag)\n",
    "      test_acc = accuracy(tag.flatten(),test_labels.flatten())\n",
    "\n",
    "      test_batch_loss += test_loss\n",
    "      test_batch_acc += test_acc\n",
    "\n",
    "    test_batch_loss /= len(test_dataset_batched)\n",
    "    test_batch_acc /= len(test_dataset_batched)\n",
    "\n",
    "    print(f'Test Loss: {test_batch_loss} | Test Accuracy: {test_batch_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tag_Sentence(sentence,model):\n",
    "  sentence = text_tokenizer.tokenize([sentence],80,text_vocab)\n",
    "  sentence = torch.tensor(sentence)\n",
    "\n",
    "  prediction = model(sentence)\n",
    "  tags = prediction.argmax(-1)\n",
    "  tags = np.array(tags)\n",
    "\n",
    "  words = [inverse_text_vocab[token] for token in np.array(sentence[0]) if  token != 0]\n",
    "  tags = [inverse_tag_vocab[tags[0][token_idx]] for token_idx in range(len(words))]\n",
    "  \n",
    "  taged_sentence = np.array(list(zip(words,tags)))\n",
    "  \n",
    "  return taged_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = Text_Tokenizer(nlp)\n",
    "tags_tokenizer = Text_Tokenizer(nlp)\n",
    "\n",
    "tokenized_train_text = text_tokenizer.tokenize(train_text,80)\n",
    "tokenized_train_tags = tags_tokenizer.tokenize(train_tags,80)\n",
    "\n",
    "text_vocab = text_tokenizer.vocab\n",
    "inverse_text_vocab = {value:key for key,value in text_vocab.get_stoi().items()}\n",
    "\n",
    "tags_vocab = tags_tokenizer.vocab\n",
    "inverse_tag_vocab = {value:key for key,value in tags_vocab.get_stoi().items()}\n",
    "\n",
    "tokenized_test_text = text_tokenizer.tokenize(test_text,80,text_vocab)\n",
    "tokenized_test_tags = tags_tokenizer.tokenize(test_tags,80,tags_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset_batched = DataLoader(list(zip(tokenized_train_text,tokenized_train_tags)),batch_size = batch_size,shuffle = True)\n",
    "test_dataset_batched = DataLoader(list(zip(tokenized_test_text,tokenized_test_tags)),batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_text,Test_tags = next(iter(train_dataset_batched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 80\n",
    "text_vocab_size = len(text_vocab.get_stoi())\n",
    "embedding_size = 164\n",
    "hidden_size = 164\n",
    "num_layers = 1\n",
    "tags_vocab_size = len(tags_vocab.get_stoi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = Tagging_rnn(sequence_length,text_vocab_size,embedding_size,hidden_size,num_layers,tags_vocab_size)\n",
    "model_lstm = Tagging_lstm(sequence_length,text_vocab_size,embedding_size,hidden_size,num_layers,tags_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN model: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60556c673a9c4d3a84742cf287e0a0ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79359b2afd7240f7b26014f4587fc6da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train Loss: 1.1571853160858154 | Train Accuracy: 0.7734371464059415\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61526376b1446c4bcfc39e7f2d894f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train Loss: 0.530951738357544 | Train Accuracy: 0.8597926267281105\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9df95ace8534d6ea66cecea0ca2b118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Train Loss: 0.4284038543701172 | Train Accuracy: 0.8882275037030944\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f882b2fe8b0f4c22a9a5a472644fa005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Train Loss: 0.35990628600120544 | Train Accuracy: 0.9002603738067816\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b492edcd635a4d9aa6b467d4d81663fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Train Loss: 0.30723732709884644 | Train Accuracy: 0.9120598075419679\n",
      "Test Loss: 0.2282353639602661 | Test Accuracy: 0.9361283570954905\n",
      "LSTM model: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86a512996ad4b4eaeacedfb90c93794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e142908d5bde47229dc2e52b51af2b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train Loss: 1.352070689201355 | Train Accuracy: 0.6839395649893024\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ae77c3e61f4a72b46db321113a072a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train Loss: 0.5569143295288086 | Train Accuracy: 0.8494644978707203\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a09e16d8f74c4bb0ddb69988fbf1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Train Loss: 0.4502781331539154 | Train Accuracy: 0.8757764925526667\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07f5a2c5cdb42acb03fe299ef90e12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Train Loss: 0.37023913860321045 | Train Accuracy: 0.9026095884422314\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e64989900d46a189f08ddfe2136c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Train Loss: 0.30035918951034546 | Train Accuracy: 0.9183307660261684\n",
      "Test Loss: 0.21578410267829895 | Test Accuracy: 0.9419512599469493\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer_lstm = torch.optim.Adam(model_lstm.parameters(),lr = lr)\n",
    "optimizer_rnn = torch.optim.Adam(model_rnn.parameters(),lr = lr)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "Trainer_rnn = Train_Model(model_rnn,loss_function,optimizer_rnn,epochs)\n",
    "Trainer_lstm = Train_Model(model_lstm,loss_function,optimizer_lstm,epochs)\n",
    "\n",
    "print(f'RNN model: ')\n",
    "Trainer_rnn.fit(train_dataset_batched)\n",
    "Trainer_rnn.eval(test_dataset_batched)\n",
    "\n",
    "\n",
    "print(f'LSTM model: ')\n",
    "Trainer_lstm.fit(train_dataset_batched)\n",
    "Trainer_lstm.eval(test_dataset_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['<unk>', 'VERB'],\n",
       "       ['can', 'AUX'],\n",
       "       ['anyone', 'VERB'],\n",
       "       ['use', 'VERB'],\n",
       "       ['military', 'NOUN'],\n",
       "       ['pressure', 'NOUN'],\n",
       "       ['without', 'NOUN'],\n",
       "       ['proof', 'NOUN'],\n",
       "       ['?', 'PUNCT']], dtype='<U8')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = test_text[34]\n",
    "\n",
    "Tag_Sentence(sentence,model_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
