{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 19:57:39.732252: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               I`d have responded, if I were going\n",
       "1     Sooo SAD I will miss you here in San Diego!!!\n",
       "2                         my boss is bullying me...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/nazarlenisin/Desktop/Text Generation/Tweets.csv')\n",
    "df = df['text']\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ' '.join(df.values.astype(str))\n",
    "data = [token.lower() for token in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenize_Text:\n",
    "  def __init__(self,nlp):\n",
    "    self.nlp = nlp\n",
    "\n",
    "  def _yield_tokens(self,text):\n",
    "    for token in text:\n",
    "      yield token\n",
    "\n",
    "  def _vocab(self,text):\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        self._yield_tokens(text),\n",
    "        specials = ['<unk>']\n",
    "    )\n",
    "\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "    self.vocab = vocab\n",
    "\n",
    "    return vocab\n",
    "\n",
    "  def tokenize(self,text,vocab = None):\n",
    "    tokens = [token for token in text]\n",
    "\n",
    "    if vocab == None:\n",
    "      vocab = self._vocab(text)\n",
    "\n",
    "    transforms = T.Sequential(\n",
    "        T.VocabTransform(vocab),\n",
    "        T.ToTensor()\n",
    "    )\n",
    "\n",
    "    tokenized_text = transforms(tokens)\n",
    "\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windows(data,window_size):\n",
    "  windows = []\n",
    "\n",
    "  index = 0\n",
    "  window_size += 1\n",
    "\n",
    "  while index + window_size <= len(data) - 1:\n",
    "    windows.append(data[index: index +window_size])\n",
    "\n",
    "    index += 1\n",
    "\n",
    "  return np.array(windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generative_lstm(torch.nn.Module):\n",
    "  def __init__(self,sequence_length,hidden_size,num_layers,vocab_size):\n",
    "    super().__init__()\n",
    "    self.sequence_length = sequence_length\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "    self.vocab_size  = vocab_size\n",
    "\n",
    "    self.lstm = torch.nn.LSTM(vocab_size,hidden_size,num_layers,batch_first = True)\n",
    "    self.flatten = torch.nn.Flatten()\n",
    "    self.linear = torch.nn.Linear(hidden_size,vocab_size)\n",
    "\n",
    "  def forward(self,X):\n",
    "    X = torch.nn.functional.one_hot(X,num_classes = self.vocab_size)\n",
    "    X = X.to(torch.float32)\n",
    "    X,_ = self.lstm(X)\n",
    "    X = self.linear(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Model:\n",
    "  def __init__(self,model,loss_function,optimizer,epochs = 5):\n",
    "    self.model = model\n",
    "    self.loss_function = loss_function\n",
    "    self.optimizer = optimizer\n",
    "    self.epochs = epochs\n",
    "\n",
    "  def fit(self,train_data_batched):\n",
    "    from tqdm.auto import tqdm\n",
    "    from sklearn.metrics import accuracy_score as accuracy\n",
    "\n",
    "    model = self.model\n",
    "    model.train()\n",
    "\n",
    "    train_batch_loss = 0\n",
    "    train_batch_acc = 0\n",
    "\n",
    "    for epoch in tqdm(range(self.epochs)):\n",
    "      print(f'\\n\\nEpoch: {epoch}')\n",
    "\n",
    "      for batch,(X,y) in tqdm(enumerate(train_data_batched)):\n",
    "        if batch % 100 == 0: print(f'\\nBatch {batch} / {len(train_data_batched)}')\n",
    "\n",
    "        train_prediction = model(X)\n",
    "        train_labels = train_prediction.argmax(-1)\n",
    "\n",
    "        train_loss = self.loss_function(train_prediction.permute(0,2,1),y)\n",
    "        train_acc = accuracy(y.flatten(),train_labels.flatten())\n",
    "\n",
    "        train_batch_loss += train_loss\n",
    "        train_batch_acc += train_acc\n",
    "        if batch % 100 == 0: print(f'Train Loss: {train_loss} | Train Accuracy: {train_acc}')\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if batch == 1875: break\n",
    "\n",
    "\n",
    "      train_batch_loss /= 1875\n",
    "      train_batch_acc /= 1875\n",
    "\n",
    "      print(f'Epoch: {epoch} | Train Loss: {train_batch_loss} | Train Accuracy: {train_batch_acc}')\n",
    "      \n",
    "    return model\n",
    "\n",
    "  def eval(self,test_data_batched):\n",
    "    from tqdm.auto import tqdm\n",
    "    from sklearn.metrics import accuracy_score as accuracy\n",
    "\n",
    "    model = self.model\n",
    "    model.eval\n",
    "\n",
    "    test_batch_loss = 0\n",
    "    test_batch_acc = 0\n",
    "\n",
    "    for batch,(X,y) in tqdm(enumerate(test_data_batched)):\n",
    "      if batch % 10 == 0: print(f'\\nBatch {batch} / {len(test_data_batched)}')\n",
    "\n",
    "      test_prediction = model(X)\n",
    "      test_labels = test_prediction.argmax(-1)\n",
    "\n",
    "      test_loss = self.loss_function(test_prediction.permute(0,2,1),y)\n",
    "      test_acc = accuracy(y.flatten(),test_labels.flatten())\n",
    "      if batch % 100 == 0: print(f'Test Loss: {test_loss} | Test Accuracy: {test_acc}')\n",
    "\n",
    "      test_batch_loss += test_loss\n",
    "      test_batch_acc += test_acc\n",
    "\n",
    "      if batch == 1875: break\n",
    "\n",
    "\n",
    "    test_batch_loss /= 1875\n",
    "    test_batch_acc /= 1875\n",
    "\n",
    "    print(f'\\nTest Loss: {test_batch_loss} | Test Accuracy: {test_batch_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,vocab,inverse_vocab, tokenizer, seed_text, num_chars=200, temperature= 0.001):\n",
    "\n",
    "  text = seed_text  \n",
    "\n",
    "  for _ in range(num_chars):\n",
    "    input = np.array(tokenizer.tokenize(text[-101:],vocab))\n",
    "\n",
    "    preds = model(torch.tensor(input).unsqueeze(0))[0, -1:, :]\n",
    "    preds = tf.constant(torch.softmax(preds,-1).detach().numpy())\n",
    "    \n",
    "    preds = tf.math.log(preds) / temperature\n",
    "    \n",
    "    next_char = tf.random.categorical(preds, num_samples=1)\n",
    "    next_char = inverse_vocab[next_char.numpy()[0][0]]\n",
    "    \n",
    "\n",
    "    text += next_char\n",
    "\n",
    "  return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenizer = Tokenize_Text(nlp)\n",
    "\n",
    "tokenized_data = np.array(Tokenizer.tokenize(data))\n",
    "\n",
    "vocab = Tokenizer.vocab\n",
    "inverse_vocab = {value:key for key,value in vocab.get_stoi().items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('  i ` d   h a v e  ', 'i ` d   h a v e   r')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliced_data = windows(tokenized_data,101)\n",
    "\n",
    "X = sliced_data[:,:-1]\n",
    "y = sliced_data[:,1:]\n",
    "\n",
    "' '.join([inverse_vocab[token] for token in X[0][:10]]),' '.join([inverse_vocab[token] for token in y[0][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 101]), torch.Size([32, 101]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "batched_data = DataLoader(list(zip(X,y)),batch_size = batch_size)\n",
    "\n",
    "x,y = next(iter(batched_data))\n",
    "\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = len(X[0])\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "vocab_size = len(vocab.get_stoi())\n",
    "\n",
    "model = Generative_lstm(sequence_length,hidden_size,num_layers,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd40018953f34be19143604c3fea0c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a51477926d44c1a920157820242e154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 0 / 59535\n",
      "Train Loss: 1.208869218826294 | Train Accuracy: 0.6367574257425742\n",
      "\n",
      "Batch 100 / 59535\n",
      "Train Loss: 0.8957428932189941 | Train Accuracy: 0.7283415841584159\n",
      "\n",
      "Batch 200 / 59535\n",
      "Train Loss: 0.7051419615745544 | Train Accuracy: 0.786819306930693\n",
      "\n",
      "Batch 300 / 59535\n",
      "Train Loss: 0.7147476077079773 | Train Accuracy: 0.8115717821782178\n",
      "\n",
      "Batch 400 / 59535\n",
      "Train Loss: 0.6015011072158813 | Train Accuracy: 0.8301361386138614\n",
      "\n",
      "Batch 500 / 59535\n",
      "Train Loss: 0.6922172904014587 | Train Accuracy: 0.7933168316831684\n",
      "\n",
      "Batch 600 / 59535\n",
      "Train Loss: 0.6821919679641724 | Train Accuracy: 0.7920792079207921\n",
      "\n",
      "Batch 700 / 59535\n",
      "Train Loss: 0.7874571681022644 | Train Accuracy: 0.8050742574257426\n",
      "\n",
      "Batch 800 / 59535\n",
      "Train Loss: 0.8138601183891296 | Train Accuracy: 0.7057549504950495\n",
      "\n",
      "Batch 900 / 59535\n",
      "Train Loss: 0.5656724572181702 | Train Accuracy: 0.8418935643564357\n",
      "\n",
      "Batch 1000 / 59535\n",
      "Train Loss: 0.7411655187606812 | Train Accuracy: 0.8143564356435643\n",
      "\n",
      "Batch 1100 / 59535\n",
      "Train Loss: 0.5551716089248657 | Train Accuracy: 0.8573638613861386\n",
      "\n",
      "Batch 1200 / 59535\n",
      "Train Loss: 0.6692559123039246 | Train Accuracy: 0.807240099009901\n",
      "\n",
      "Batch 1300 / 59535\n",
      "Train Loss: 0.6198495626449585 | Train Accuracy: 0.8100247524752475\n",
      "\n",
      "Batch 1400 / 59535\n",
      "Train Loss: 0.6867848634719849 | Train Accuracy: 0.7778465346534653\n",
      "\n",
      "Batch 1500 / 59535\n",
      "Train Loss: 0.9606126546859741 | Train Accuracy: 0.692450495049505\n",
      "\n",
      "Batch 1600 / 59535\n",
      "Train Loss: 0.7120466828346252 | Train Accuracy: 0.8029084158415841\n",
      "\n",
      "Batch 1700 / 59535\n",
      "Train Loss: 0.7060448527336121 | Train Accuracy: 0.7611386138613861\n",
      "\n",
      "Batch 1800 / 59535\n",
      "Train Loss: 0.7429028749465942 | Train Accuracy: 0.7441212871287128\n",
      "Epoch: 0 | Train Loss: 0.7307794094085693 | Train Accuracy: 0.7817630363036302\n",
      "\n",
      "\n",
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ad2a9b0eee4479bb929e05f62d6ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 0 / 59535\n",
      "Train Loss: 1.1865246295928955 | Train Accuracy: 0.6466584158415841\n",
      "\n",
      "Batch 100 / 59535\n",
      "Train Loss: 0.7658790349960327 | Train Accuracy: 0.7753712871287128\n",
      "\n",
      "Batch 200 / 59535\n",
      "Train Loss: 0.6792097091674805 | Train Accuracy: 0.7722772277227723\n",
      "\n",
      "Batch 300 / 59535\n",
      "Train Loss: 0.6068872213363647 | Train Accuracy: 0.84375\n",
      "\n",
      "Batch 400 / 59535\n",
      "Train Loss: 0.5874357223510742 | Train Accuracy: 0.8087871287128713\n",
      "\n",
      "Batch 500 / 59535\n",
      "Train Loss: 0.7280972003936768 | Train Accuracy: 0.791769801980198\n",
      "\n",
      "Batch 600 / 59535\n",
      "Train Loss: 0.7066624760627747 | Train Accuracy: 0.7663985148514851\n",
      "\n",
      "Batch 700 / 59535\n",
      "Train Loss: 0.7833223938941956 | Train Accuracy: 0.7626856435643564\n",
      "\n",
      "Batch 800 / 59535\n",
      "Train Loss: 0.8557454943656921 | Train Accuracy: 0.7212252475247525\n",
      "\n",
      "Batch 900 / 59535\n",
      "Train Loss: 0.5604797005653381 | Train Accuracy: 0.8313737623762376\n",
      "\n",
      "Batch 1000 / 59535\n",
      "Train Loss: 0.75356125831604 | Train Accuracy: 0.7685643564356436\n",
      "\n",
      "Batch 1100 / 59535\n",
      "Train Loss: 0.5864979028701782 | Train Accuracy: 0.8446782178217822\n",
      "\n",
      "Batch 1200 / 59535\n",
      "Train Loss: 0.7139158844947815 | Train Accuracy: 0.7911509900990099\n",
      "\n",
      "Batch 1300 / 59535\n",
      "Train Loss: 0.650238573551178 | Train Accuracy: 0.7704207920792079\n",
      "\n",
      "Batch 1400 / 59535\n",
      "Train Loss: 0.6830027103424072 | Train Accuracy: 0.7710396039603961\n",
      "\n",
      "Batch 1500 / 59535\n",
      "Train Loss: 0.8463736176490784 | Train Accuracy: 0.7744430693069307\n",
      "\n",
      "Batch 1600 / 59535\n",
      "Train Loss: 0.6920722723007202 | Train Accuracy: 0.7899133663366337\n",
      "\n",
      "Batch 1700 / 59535\n",
      "Train Loss: 0.7268352508544922 | Train Accuracy: 0.7561881188118812\n",
      "\n",
      "Batch 1800 / 59535\n",
      "Train Loss: 0.685725212097168 | Train Accuracy: 0.8029084158415841\n",
      "Epoch: 1 | Train Loss: 0.7055742740631104 | Train Accuracy: 0.7927661152035208\n",
      "\n",
      "\n",
      "Epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d746235b4b3e44f5a9a9c616b147812d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 0 / 59535\n",
      "Train Loss: 1.1141180992126465 | Train Accuracy: 0.6751237623762376\n",
      "\n",
      "Batch 100 / 59535\n",
      "Train Loss: 0.7358376383781433 | Train Accuracy: 0.7892945544554455\n",
      "\n",
      "Batch 200 / 59535\n",
      "Train Loss: 0.6502729654312134 | Train Accuracy: 0.7645420792079208\n",
      "\n",
      "Batch 300 / 59535\n",
      "Train Loss: 0.6102358102798462 | Train Accuracy: 0.8248762376237624\n",
      "\n",
      "Batch 400 / 59535\n",
      "Train Loss: 0.6237249970436096 | Train Accuracy: 0.8155940594059405\n",
      "\n",
      "Batch 500 / 59535\n",
      "Train Loss: 0.6635831594467163 | Train Accuracy: 0.8081683168316832\n",
      "\n",
      "Batch 600 / 59535\n",
      "Train Loss: 0.5957456827163696 | Train Accuracy: 0.8270420792079208\n",
      "\n",
      "Batch 700 / 59535\n",
      "Train Loss: 0.6948043704032898 | Train Accuracy: 0.8449876237623762\n",
      "\n",
      "Batch 800 / 59535\n",
      "Train Loss: 0.7559986114501953 | Train Accuracy: 0.7663985148514851\n",
      "\n",
      "Batch 900 / 59535\n",
      "Train Loss: 0.5919710993766785 | Train Accuracy: 0.8270420792079208\n",
      "\n",
      "Batch 1000 / 59535\n",
      "Train Loss: 0.7202109694480896 | Train Accuracy: 0.7725866336633663\n",
      "\n",
      "Batch 1100 / 59535\n",
      "Train Loss: 0.5895297527313232 | Train Accuracy: 0.8199257425742574\n",
      "\n",
      "Batch 1200 / 59535\n",
      "Train Loss: 0.7354643940925598 | Train Accuracy: 0.7797029702970297\n",
      "\n",
      "Batch 1300 / 59535\n",
      "Train Loss: 0.6456565260887146 | Train Accuracy: 0.78125\n",
      "\n",
      "Batch 1400 / 59535\n",
      "Train Loss: 0.746559202671051 | Train Accuracy: 0.7487623762376238\n",
      "\n",
      "Batch 1500 / 59535\n",
      "Train Loss: 0.7977777123451233 | Train Accuracy: 0.7806311881188119\n",
      "\n",
      "Batch 1600 / 59535\n",
      "Train Loss: 0.6239604949951172 | Train Accuracy: 0.8118811881188119\n",
      "\n",
      "Batch 1700 / 59535\n",
      "Train Loss: 0.665391206741333 | Train Accuracy: 0.8220915841584159\n",
      "\n",
      "Batch 1800 / 59535\n",
      "Train Loss: 0.7036316990852356 | Train Accuracy: 0.8152846534653465\n",
      "Epoch: 2 | Train Loss: 0.6946322321891785 | Train Accuracy: 0.7943690132152376\n",
      "\n",
      "\n",
      "Epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5eebd7f4dd94e88ab54ee2315439bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 0 / 59535\n",
      "Train Loss: 1.1793403625488281 | Train Accuracy: 0.6584158415841584\n",
      "\n",
      "Batch 100 / 59535\n",
      "Train Loss: 0.8160816431045532 | Train Accuracy: 0.786200495049505\n",
      "\n",
      "Batch 200 / 59535\n",
      "Train Loss: 0.6410121321678162 | Train Accuracy: 0.8208539603960396\n",
      "\n",
      "Batch 300 / 59535\n",
      "Train Loss: 0.6469395756721497 | Train Accuracy: 0.8224009900990099\n",
      "\n",
      "Batch 400 / 59535\n",
      "Train Loss: 0.527975857257843 | Train Accuracy: 0.8412747524752475\n",
      "\n",
      "Batch 500 / 59535\n",
      "Train Loss: 0.6775121688842773 | Train Accuracy: 0.7663985148514851\n",
      "\n",
      "Batch 600 / 59535\n",
      "Train Loss: 0.5775192379951477 | Train Accuracy: 0.7985767326732673\n",
      "\n",
      "Batch 700 / 59535\n",
      "Train Loss: 0.7206678986549377 | Train Accuracy: 0.8081683168316832\n",
      "\n",
      "Batch 800 / 59535\n",
      "Train Loss: 0.7010076642036438 | Train Accuracy: 0.7540222772277227\n",
      "\n",
      "Batch 900 / 59535\n",
      "Train Loss: 0.5035039186477661 | Train Accuracy: 0.880569306930693\n",
      "\n",
      "Batch 1000 / 59535\n",
      "Train Loss: 0.6946307420730591 | Train Accuracy: 0.7790841584158416\n",
      "\n",
      "Batch 1100 / 59535\n",
      "Train Loss: 0.5773632526397705 | Train Accuracy: 0.8236386138613861\n",
      "\n",
      "Batch 1200 / 59535\n",
      "Train Loss: 0.7940059900283813 | Train Accuracy: 0.7438118811881188\n",
      "\n",
      "Batch 1300 / 59535\n",
      "Train Loss: 0.6317636370658875 | Train Accuracy: 0.7781559405940595\n",
      "\n",
      "Batch 1400 / 59535\n",
      "Train Loss: 0.650445818901062 | Train Accuracy: 0.7676361386138614\n",
      "\n",
      "Batch 1500 / 59535\n",
      "Train Loss: 0.7647039294242859 | Train Accuracy: 0.7753712871287128\n",
      "\n",
      "Batch 1600 / 59535\n",
      "Train Loss: 0.6874114871025085 | Train Accuracy: 0.7750618811881188\n",
      "\n",
      "Batch 1700 / 59535\n",
      "Train Loss: 0.7225072979927063 | Train Accuracy: 0.7694925742574258\n",
      "\n",
      "Batch 1800 / 59535\n",
      "Train Loss: 0.6486310958862305 | Train Accuracy: 0.8007425742574258\n",
      "Epoch: 3 | Train Loss: 0.6876247525215149 | Train Accuracy: 0.796953696477014\n",
      "\n",
      "\n",
      "Epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a5f7b24f7f4b859a9a68a28dcf3702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 0 / 59535\n",
      "Train Loss: 1.0345689058303833 | Train Accuracy: 0.6714108910891089\n",
      "\n",
      "Batch 100 / 59535\n",
      "Train Loss: 0.766114354133606 | Train Accuracy: 0.7818688118811881\n",
      "\n",
      "Batch 200 / 59535\n",
      "Train Loss: 0.6211530566215515 | Train Accuracy: 0.8360148514851485\n",
      "\n",
      "Batch 300 / 59535\n",
      "Train Loss: 0.5934463739395142 | Train Accuracy: 0.8295173267326733\n",
      "\n",
      "Batch 400 / 59535\n",
      "Train Loss: 0.5522067546844482 | Train Accuracy: 0.8431311881188119\n",
      "\n",
      "Batch 500 / 59535\n",
      "Train Loss: 0.7152782678604126 | Train Accuracy: 0.7512376237623762\n",
      "\n",
      "Batch 600 / 59535\n",
      "Train Loss: 0.5462782979011536 | Train Accuracy: 0.8159034653465347\n",
      "\n",
      "Batch 700 / 59535\n",
      "Train Loss: 0.653044581413269 | Train Accuracy: 0.8329207920792079\n",
      "\n",
      "Batch 800 / 59535\n",
      "Train Loss: 0.7442502975463867 | Train Accuracy: 0.786819306930693\n",
      "\n",
      "Batch 900 / 59535\n",
      "Train Loss: 0.5709152221679688 | Train Accuracy: 0.8369430693069307\n",
      "\n",
      "Batch 1000 / 59535\n",
      "Train Loss: 0.6724869608879089 | Train Accuracy: 0.8214727722772277\n",
      "\n",
      "Batch 1100 / 59535\n",
      "Train Loss: 0.5130756497383118 | Train Accuracy: 0.8301361386138614\n",
      "\n",
      "Batch 1200 / 59535\n",
      "Train Loss: 0.7209288477897644 | Train Accuracy: 0.7654702970297029\n",
      "\n",
      "Batch 1300 / 59535\n",
      "Train Loss: 0.5820232629776001 | Train Accuracy: 0.849009900990099\n",
      "\n",
      "Batch 1400 / 59535\n",
      "Train Loss: 0.5909379720687866 | Train Accuracy: 0.8044554455445545\n",
      "\n",
      "Batch 1500 / 59535\n",
      "Train Loss: 0.7774684429168701 | Train Accuracy: 0.7564975247524752\n",
      "\n",
      "Batch 1600 / 59535\n",
      "Train Loss: 0.6263826489448547 | Train Accuracy: 0.8118811881188119\n",
      "\n",
      "Batch 1700 / 59535\n",
      "Train Loss: 0.6592063307762146 | Train Accuracy: 0.8100247524752475\n",
      "\n",
      "Batch 1800 / 59535\n",
      "Train Loss: 0.7019792795181274 | Train Accuracy: 0.7858910891089109\n",
      "Epoch: 4 | Train Loss: 0.6800131797790527 | Train Accuracy: 0.7990961640836658\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = lr)\n",
    "epochs = 5\n",
    "\n",
    "Trainer = Train_Model(model,loss_function,optimizer,epochs = epochs)\n",
    "\n",
    "model = Trainer.fit(batched_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,vocab,inverse_vocab, tokenizer, seed_text, num_chars=200, temperature= 0.001):\n",
    "\n",
    "  text = seed_text  \n",
    "\n",
    "  for _ in range(num_chars):\n",
    "    input = np.array(tokenizer.tokenize(text[-101:],vocab))\n",
    "\n",
    "    preds = model(torch.tensor(input).unsqueeze(0))[0, -1:, :]\n",
    "    preds = tf.constant(torch.softmax(preds,-1).detach().numpy())\n",
    "    \n",
    "    preds = tf.math.log(preds) / temperature\n",
    "    \n",
    "    next_char = tf.random.categorical(preds, num_samples=1)\n",
    "    next_char = inverse_vocab[next_char.numpy()[0][0]]\n",
    "    \n",
    "\n",
    "    text += next_char\n",
    "\n",
    "  return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i had responded to put much is bad shates later and now much i just again it hell  i`ll now   ohh? i hade ir arast hav'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model ,vocab,inverse_vocab,Tokenizer,'i had responded to', num_chars = 100, temperature = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
